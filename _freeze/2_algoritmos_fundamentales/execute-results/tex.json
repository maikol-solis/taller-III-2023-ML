{
  "hash": "8771b3bbca17df7834e9d426b136326d",
  "result": {
    "markdown": "# Día 2\n\n## Regresión Lineal \n\n## Regresión Logística\n\n## Decision Trees\n\n### Definición\n\nUn árbol de decisión (DT) es un grafo no cíclico que se utiliza para tomar decisiones (clasificar). En cada nodo (rama) del grafo se evalúa uno de los *features*. Si el resultado de la evaluación es cierto (o está debajo de un umbral), se sigue la rama de la izquierda, si no se va a la derecha.\n\n![](./figuras/DT01.svg){fig-alt=\"Un árbol de decisión.\" fig-align=\"center\"}\n\nPor lo tanto, los DT son un modelo no paramétrico.\n\nPara crear el DT, se **intenta** optimizar el promedio de la máxima verosimilitud:\n$$\n    \\frac{1}{N} \\sum_{i=1}^{N}\\left( y_i \\ln{f_{ID3}(x_i)} + (1-y_i) \\ln{(1-f_{ID3}(x_i))}\\right) \n$$\ndonde $f_{ID3}$ es un DT y $f_{ID3}(x) \\stackrel{\\text{def}}{=} Pr(y=1|x)$\n\n### Construcción\nPara construir el árbol, en cada nodo de decisión, se intenta minimizar la entropía de la información.\n\nLa entropía de un conjunto $\\cal{S}$ viene dada por:\n$$\n H(S) \\stackrel{\\text{def}}{=} -f_{ID3}^{S} \\log_2 (f_{ID3}^{S}) - (1-f_{ID3}^{S}) \\log_2 (1-f_{ID3}^{S})\n$$\n\nY si un grupo se divide en dos, la entropía es la suma ponderada de cada subconjunto:\n$$\n  H(S_-, S_+) \\stackrel{\\text{def}}{=} \\frac{|S_-|}{|S|}H(S_-) + \\frac{|S_+|}{|S|}H(S_+)\n$$\n\n### Ejemplo\nConsideremos los siguientes datos:\n\nAtributos:\n\n* Edad: viejo (v), media-vida(m), nuevo (nv)\n* Competencia: no(n), sí(s)\n* Tipo: software (swr), hardware (hwr)\n\n| Edad | Competencia | Tipo | Ganancia |\n|------|-------------|------|----------|\n| v    | s           | swr  | baja     |\n| v    | n           | swr  | baja     |\n| v    | n           | hwr  | baja     |\n| m    | s           | swr  | baja     |\n| m    | s           | hwr  | baja     |\n| m    | n           | hwr  | sube     |\n| m    | n           | swr  | sube     |\n| nv   | s           | swr  | sube     |\n| nv   | n           | hwr  | sube     |\n| nv   | n           | swr  | sube     |\n\nCálculo de las entropías: Primero se tiene que probar todos los features para ver cuál tiene mayor ganancia de información (reduce la entropía)\n\nEntropía total:\n$$\nH(S) = \\text{Entropía de los casos baja} + \\text{Entropía de los casos sube}\n$$\n\n$$\nH(s) = -\\frac{5}{10}*\\log_2(\\frac{5}{10}) - \\frac{5}{10}*\\log_2(\\frac{5}{10}) = 1\n$$\n\nAhora vamos a decidir la primera separación con las edades\n![](./figuras/DT02.svg){fig-align=\"center\"}\n$H = \\frac{3}{10} \\cdot 0 + \\frac{4}{10} \\cdot 1 + \\frac{3}{10} \\cdot 0 = 0.4$\n\nAhora vamos a decidir la primera separación con la competencia\n![](./figuras/DT03.svg){fig-align=\"center\"}\n$H = \\frac{4}{10} \\cdot 0.811 + \\frac{6}{10} \\cdot 0.918 = 0.8752$\n\nAhora vamos a decidir la primera separación con las edades\n![](./figuras/DT03.svg){fig-align=\"center\"}\n$H = \\frac{4}{10} \\cdot 0.811 + \\frac{6}{10} \\cdot 0.918 = 0.8752$\n\nAhora vamos a decidir la primera separación con el tipo\n![](./figuras/DT04.svg){fig-align=\"center\"}\n$H = \\frac{6}{10} \\cdot 1 + \\frac{4}{10} \\cdot 1 = 1$\n\nConcluimos que lo que nos da la máxima ganancia de información es primero decidir por edades, eso nos deja dos nodos hoja y un nodo rama que debemos volver a separar.\n\nAhora vamos a buscar el segundo nivel, donde vamos a separar el grupo que tiene edades medias por competencia:\n\n![](./figuras/DT05.svg){fig-align=\"center\"}\n$H = \\frac{2}{4} \\cdot 0 + \\frac{2}{4} \\cdot 0 = 0$\n\nCon esto ya se clasificaron todos los datos, puesto que terminamos solo con nodos hojas:\n![](./figuras/DT06.svg){fig-align=\"center\"}\n\nEsto también se puede hacer con valores numéricos, que de hecho, es lo que se puede hacer con scikit learn\n![](./figuras/DT11.svg){fig-align=\"center\"}\n\ny con esto se obtiene este árbol de decisión:\n![](./figuras/DT12.svg){fig-align=\"center\"}\n\n### Comandos básicos en python\nEstos son los comandos básicos en python\n\n```python\n#| label: dibujoArbol01\n#| fig-cap: \"Árbol de decisión\"\nfrom sklearn import tree\nX = # Lista con los features (lista de listas)\nY = # Lista con los labels\n# Se define la variable que tendrá el árbol\nclf = tree.DecisionTreeClassifier()\n# Se calcula el árbol\nclf = clf.fit(X, Y)\n# Se utiliza el árbol para predecir el label de un dato nuevo\nclf.predict_proba(X0)\n# Se dibuja el árbol\ntree.plot_tree(clf)\n```\ny este sería un ejemplo sencillo en python:\n\nPrimero creamos los datos\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom sklearn import tree\nfrom sklearn.datasets import make_blobs\nfrom sklearn.inspection import DecisionBoundaryDisplay\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Creación de los datos\nX, Y = make_blobs(n_samples=200, centers=4, random_state=6)\nplt.scatter(X[:, 0], X[:, 1], c=Y, s=30)\nplt.title(\"Datos originales\")\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Ejemplo hecho en python: datos](2_algoritmos_fundamentales_files/figure-pdf/ejemplo01datos-output-1.pdf){#ejemplo01datos fig-pos='H'}\n:::\n:::\n\n\nLuego se crea el arbol\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(X, Y)\ntree.plot_tree(clf)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Ejemplo hecho en python: arbol](2_algoritmos_fundamentales_files/figure-pdf/ejemplo01arbol-output-1.pdf){#ejemplo01arbol fig-pos='H'}\n:::\n:::\n\n\ny por último, dibujamos las separaciones\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nDecisionBoundaryDisplay.from_estimator(clf, X, response_method=\"predict\")\nplt.scatter(X[:, 0], X[:, 1], c=Y, s=30)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Ejemplo hecho en python: separación](2_algoritmos_fundamentales_files/figure-pdf/ejemplo01separacion-output-1.pdf){#ejemplo01separacion fig-pos='H'}\n:::\n:::\n\n\ny con esto se puede aplicar el árbol\n\n::: {#ejemplo01aplicacion .cell execution_count=4}\n``` {.python .cell-code}\nprint(clf.predict([[5.0, 1.0]]))\nprint(clf.predict([[-2.0, -1.0]]))\nprint(clf.predict([[6.0, -6.0]]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0]\n[3]\n[0]\n```\n:::\n:::\n\n\ny lo que devuelve es el número de grupo al que pertene el dato\n\n\n\n## K-Nearest Neighbors (KNN)\n\n\n### Carga de paquetes\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import make_blobs\n```\n:::\n\n\n# Cargas datos\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nX, y = make_blobs(n_samples=1000, centers=3, random_state=6)\n```\n:::\n\n\n# Visualizar los datos\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nsns.scatterplot(x=X[:,0],y=X[:,1], hue=y)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](2_algoritmos_fundamentales_files/figure-pdf/cell-8-output-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n# Se normaliza y se divide los datos\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n\n# Scale the features using StandardScaler\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n```\n:::\n\n\n# Ajuste y evaluación del modelo\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\n\n# predecir con el modelo\ny_pred = knn.predict(X_test)\n\n# evaluarlo\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 1.0\n```\n:::\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nDecisionBoundaryDisplay.from_estimator(knn, X_train)\nsns.scatterplot(x=X[:,0],y=X[:,1], hue=y)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](2_algoritmos_fundamentales_files/figure-pdf/cell-11-output-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n",
    "supporting": [
      "2_algoritmos_fundamentales_files/figure-pdf"
    ],
    "filters": []
  }
}