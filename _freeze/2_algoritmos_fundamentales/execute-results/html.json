{
  "hash": "278e9cc88b67e7a5cf9fe81ea12248a0",
  "result": {
    "engine": "jupyter",
    "markdown": "# Día 2\n\n## Regresión Lineal \n\n## Regresión Logística\n\n## Decision Trees\n\n### Definición\n\nUn árbol de decisión (DT) es un grafo no cíclico que se utiliza para tomar decisiones (clasificar). En cada nodo (rama) del grafo se evalúa uno de los *features*. Si el resultado de la evaluación es cierto (o está debajo de un umbral), se sigue la rama de la izquierda, si no se va a la derecha.\n\n![](./figuras/DT01.svg){fig-alt=\"Un árbol de decisión.\" fig-align=\"center\"}\n\nPor lo tanto, los DT son un modelo no paramétrico.\n\nPara crear el DT, se **intenta** optimizar el promedio de la máxima verosimilitud:\n$$\n    \\frac{1}{N} \\sum_{i=1}^{N}\\left( y_i \\ln{f_{ID3}(x_i)} + (1-y_i) \\ln{(1-f_{ID3}(x_i))}\\right) \n$$\ndonde $f_{ID3}$ es un DT y $f_{ID3}(x) \\stackrel{\\text{def}}{=} Pr(y=1|x)$\n\n### Construcción\nPara construir el árbol, en cada nodo de decisión, se intenta minimizar la entropía de la información.\n\nLa entropía de un conjunto $\\cal{S}$ viene dada por:\n$$\n H(S) \\stackrel{\\text{def}}{=} -f_{ID3}^{S} \\log_2 (f_{ID3}^{S}) - (1-f_{ID3}^{S}) \\log_2 (1-f_{ID3}^{S})\n$$\n\nY si un grupo se divide en dos, la entropía es la suma ponderada de cada subconjunto:\n$$\n  H(S_-, S_+) \\stackrel{\\text{def}}{=} \\frac{|S_-|}{|S|}H(S_-) + \\frac{|S_+|}{|S|}H(S_+)\n$$\n\n### Ejemplo\nConsideremos los siguientes datos:\n\nAtributos:\n\n* Edad: viejo (v), media-vida(m), nuevo (nv)\n* Competencia: no(n), sí(s)\n* Tipo: software (swr), hardware (hwr)\n\n| Edad | Competencia | Tipo | Ganancia |\n|------|-------------|------|----------| \n| v\t| s\t| swr\t| baja \t| \n| v\t| n\t| swr\t| baja |\n| v\t| n\t| hwr\t| baja |\n| m\t| s\t| swr\t| baja |\n| m\t| s\t| hwr\t| baja |\n| m\t| n\t| hwr\t| sube |\n| m\t| n\t| swr\t| sube |\n| nv\t| s\t| swr\t| sube |\n| nv\t| n\t| hwr\t| sube |\n| nv\t| n\t| swr\t| sube |\n\nCálculo de las entropías: Primero se tiene que probar todos los features para ver cuál tiene mayor ganancia de información (reduce la entropía)\n\nEntropía total:\n$$\nH(S) = \\text{Entropía de los casos baja} + \\text{Entropía de los casos sube}\n$$\n\n$$\nH(s) = -\\frac{5}{10}*\\log_2(\\frac{5}{10}) - \\frac{5}{10}*\\log_2(\\frac{5}{10}) = 1\n$$\n\nAhora vamos a decidir la primera separación con las edades\n![](./figuras/DT02.svg){fig-align=\"center\"}\n$H = \\frac{3}{10} \\cdot 0 + \\frac{4}{10} \\cdot 1 + \\frac{3}{10} \\cdot 0 = 0.4$\n\nAhora vamos a decidir la primera separación con la competencia\n![](./figuras/DT03.svg){fig-align=\"center\"}\n$H = \\frac{4}{10} \\cdot 0.811 + \\frac{6}{10} \\cdot 0.918 = 0.8752$\n\nAhora vamos a decidir la primera separación con las edades\n![](./figuras/DT03.svg){fig-align=\"center\"}\n$H = \\frac{4}{10} \\cdot 0.811 + \\frac{6}{10} \\cdot 0.918 = 0.8752$\n\nAhora vamos a decidir la primera separación con el tipo\n![](./figuras/DT04.svg){fig-align=\"center\"}\n$H = \\frac{6}{10} \\cdot 1 + \\frac{4}{10} \\cdot 1 = 1$\n\nConcluimos que lo que nos da la máxima ganancia de información es primero decidir por edades, eso nos deja dos nodos hoja y un nodo rama que debemos volver a separar.\n\nAhora vamos a buscar el segundo nivel, donde vamos a separar el grupo que tiene edades medias por competencia:\n\n![](./figuras/DT05.svg){fig-align=\"center\"}\n$H = \\frac{2}{4} \\cdot 0 + \\frac{2}{4} \\cdot 0 = 0$\n\nCon esto ya se clasificaron todos los datos, puesto que terminamos solo con nodos hojas:\n![](./figuras/DT06.svg){fig-align=\"center\"}\n\nEsto también se puede hacer con valores numéricos, que de hecho, es lo que se puede hacer con scikit learn\n![](./figuras/DT11.svg){fig-align=\"center\"}\n\ny con esto se obtiene este árbol de decisión:\n![](./figuras/DT12.svg){fig-align=\"center\"}\n\n### Comandos básicos en python\nEstos son los comandos básicos en python\n\n```python\n#| label: dibujoArbol01\n#| fig-cap: \"Árbol de decisión\"\nfrom sklearn import tree\nX = # Lista con los features (lista de listas)\nY = # Lista con los labels\n# Se define la variable que tendrá el árbol\nclf = tree.DecisionTreeClassifier()\n# Se calcula el árbol\nclf = clf.fit(X, Y)\n# Se utiliza el árbol para predecir el label de un dato nuevo\nclf.predict_proba(X0)\n# Se dibuja el árbol\ntree.plot_tree(clf)\n```\ny este sería un ejemplo sencillo en python:\n\nPrimero creamos los datos\n\n::: {#cell-ejemplo01Datos .cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn import tree\nfrom sklearn.datasets import make_blobs\nfrom sklearn.inspection import DecisionBoundaryDisplay\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Creación de los datos\nX, Y = make_blobs(n_samples=200, centers=4, random_state=6)\nplt.scatter(X[:, 0], X[:, 1], c=Y, s=30)\nplt.title(\"Datos originales\")\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Ejemplo hecho en python: datos](2_algoritmos_fundamentales_files/figure-html/ejemplo01datos-output-1.png){#ejemplo01datos width=596 height=449}\n:::\n:::\n\n\nLuego se crea el arbol\n\n::: {#cell-ejemplo01Arbol .cell execution_count=3}\n``` {.python .cell-code}\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(X, Y)\ntree.plot_tree(clf)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Ejemplo hecho en python: arbol](2_algoritmos_fundamentales_files/figure-html/ejemplo01arbol-output-1.png){#ejemplo01arbol width=540 height=389}\n:::\n:::\n\n\ny por último, dibujamos las separaciones\n\n::: {#cell-ejemplo01Separacion .cell execution_count=4}\n``` {.python .cell-code}\nDecisionBoundaryDisplay.from_estimator(\n        clf,\n        X,\n        response_method=\"predict\",\n        )\nplt.scatter(X[:, 0], X[:, 1], c=Y, s=30)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Ejemplo hecho en python: separación](2_algoritmos_fundamentales_files/figure-html/ejemplo01separacion-output-1.png){#ejemplo01separacion width=577 height=411}\n:::\n:::\n\n\ny con esto se puede aplicar el árbol\n\n::: {#ejemplo01aplicacion .cell execution_count=5}\n``` {.python .cell-code}\nprint(clf.predict([[5.0, 1.0]]))\nprint(clf.predict([[-2.0, -1.0]]))\nprint(clf.predict([[6.0, -6.0]]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0]\n[3]\n[0]\n```\n:::\n:::\n\n\ny lo que devuelve es el número de grupo al que pertene el dato\n\n",
    "supporting": [
      "2_algoritmos_fundamentales_files"
    ],
    "filters": [],
    "includes": {}
  }
}